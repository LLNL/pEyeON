## Overview

This project uses a **JSON Schema** as the single source of truth for defining the structure and constraints of observation data. By maintaining the schema in one place, we ensure consistency across both application code and data storage.

### What It Does

- **Generates Python Data Classes:**  
  Automatically creates Pydantic model classes from the JSON Schema. These classes provide type-safe validation, serialization, and autocompletion within your Python code.

- **Generates Database DDL Scripts:**  
  Produces database schema (DDL) scripts that match the JSON Schema, ensuring the data stored in your database adheres to the same structure and constraints as your application code.

### Why This Approach

- **Consistency:**  
  Using the JSON Schema as the authoritative source eliminates discrepancies between code and database definitions.

- **Automation:**  
  Changes to the schema automatically propagate to both Python models and database scripts, reducing manual effort and the risk of errors.

- **Validation:**  
  Pydantic models enforce data validation at runtime, while the database schema enforces constraints at the storage layer.

- **Developer Experience:**  
  Autogenerated classes support IDE autocompletion and static analysis, making development faster and safer.

---

**In summary:**  
This workflow streamlines development by ensuring that your data definitions are always up-to-date and consistent across your Python code and database, all driven from a single, version-controlled JSON Schema file.

# Workflow Process

Typically, the workflow is simply:

1. Modify the JSON schema as needed and all unit tests pass on
    (Seth, can you provide an example of how to do this?)
2. Run `make all` in this directory, which will:
  1. `pip install datamodel-code-generator` if its not already installed
  2. Generate the `eyeon_models.py` which will have the dataclasses
  3. Generate a SQL DDL file to create necessary tables for the backend database.
3. Go back to CLI development

# Reference

## Generates a pydantic class

  ```bash
  datamodel-codegen \
  --input observation.schema.json \
  --input-file-type jsonschema \
  --target-python-version 3.11 \
  --output eyeon_models.py \
  --class-name ObservationModel \
  --use-standard-collections \
  --output-model-type pydantic_v2.BaseModel
  ```

## Example Usage

```python
from eyeon_models import ObservationModel
eyeon = ObservationModel(bytecount=5, filename='test.me', magic='ooh/magic', md5='sumhash', observation_ts='10/13/2025', sha1='shame', sha256='sha256', uuid='xyz')
```

## datamodel-codegen command overview

This command uses datamodel-code-generator to convert a JSON Schema file into Python classes targeting Pydantic v2, with specific output and type behavior.

Official documentation:
- https://koxudaxi.github.io/datamodel-code-generator/

### What the command does
- Reads your JSON Schema file, observation.schema.json.
- Generates Python model classes that conform to the schema.
- Targets Python 3.13 syntax and standard library types.
- Emits Pydantic v2 BaseModel classes for validation and serialization.
- Uses a specific top-level class name for the root schema.

### Command, broken down by argument

- `datamodel-codegen`
  - The CLI tool that generates Python models from various schema sources.

- `--input observation.schema.json`
  - Path to the source schema file.
  - Tells the generator what to parse.

- `--input-file-type jsonschema`
  - Explicitly sets the input format, helpful when the file extension is ambiguous.
  - Ensures the parser interprets the file as JSON Schema, not OpenAPI or others.

- `--target-python-version 3.11`
  - Controls the Python syntax features in the generated code.
  - For Python 3.11, you get modern typing and dataclass, typing behaviors appropriate for that version.

- `--output observation_models.py`
  - Destination file for the generated models.
  - All classes are written into this single module.

- `--class-name ObservationModel`
  - Sets the name of the root model class that represents the schema’s top-level object.
  - Useful for autocompletion and clear imports in your app code.

- `--use-standard-collections`
  - Uses built-in collection types like list and dict instead of typing.List and typing.Dict in annotations where appropriate.
  - Generally results in cleaner, more modern type hints.

- `--output-model-type pydantic_v2.BaseModel`
  - Targets Pydantic v2’s BaseModel for the generated classes.
  - Ensures compatibility with Pydantic v2 APIs, including model_dump, model_validate, and RootModel for root schemas where needed.

## gen_ddl.py overview

This script loads the JSON schema and uses that to generate corresponding SQL tables. The goal is to convert from a JSON object model to a normalized SQL model with multiple tables.

For example, the Observation table is composed of the simple, top-level fields of the JSON model. It specifically excludes the more complex fields such as metadata and certificates. Those are built as separate tables that can be joined back to observation at query time.